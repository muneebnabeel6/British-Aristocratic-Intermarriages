{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd40b58",
   "metadata": {},
   "source": [
    "# 1. DATA EXTRACTION CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4f026f",
   "metadata": {
    "id": "1a4f026f"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Optional: Provides a progress bar\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17965e52",
   "metadata": {
    "id": "17965e52"
   },
   "outputs": [],
   "source": [
    "def extract_person_occupations(xml_file):\n",
    "    occupations = []\n",
    "    with open(xml_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        start_tag = '<span role=\"occupation\">'\n",
    "        end_tag = '</span>'\n",
    "        start_index = content.find(start_tag)\n",
    "        \n",
    "        while start_index != -1:\n",
    "            start_index += len(start_tag)\n",
    "            end_index = content.find(end_tag, start_index)\n",
    "            occupation = content[start_index:end_index].strip()\n",
    "            occupations.append(occupation)\n",
    "            start_index = content.find(start_tag, end_index)\n",
    "\n",
    "    return occupations\n",
    "\n",
    "def extract_names(xml_file, print_names=False):\n",
    "    names = {\n",
    "        'Main Name': 'N/A',\n",
    "        'Forenames': 'N/A',\n",
    "        'ShortForm': 'N/A',\n",
    "        'Alternative Names': []\n",
    "    }\n",
    "\n",
    "    def extract_name(name_section, predicate):\n",
    "        name_elem = name_section.find(f\".//metaDescribes[@predicate='{predicate}']\")\n",
    "        return name_elem.text if name_elem is not None else 'N/A'\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    name_section = root.find(\".//metaItem[@about='name']\")\n",
    "    if name_section is not None:\n",
    "        names['Main Name'] = extract_name(name_section, 'mainName')\n",
    "        names['Forenames'] = extract_name(name_section, 'foreNames')\n",
    "        names['ShortForm'] = extract_name(name_section, 'ShortForm')\n",
    "    alternative_name_sections = root.findall(\".//metaItem[@about='alternative_name']\")\n",
    "    for alt_name_section in alternative_name_sections:\n",
    "        main_name = extract_name(alt_name_section, 'mainName')\n",
    "        forenames = extract_name(alt_name_section, 'foreNames')\n",
    "        if main_name != 'N/A' and main_name not in names['Alternative Names'] and main_name != names['Main Name']:\n",
    "            names['Alternative Names'].append(main_name)\n",
    "        if forenames != 'N/A' and forenames not in names['Alternative Names'] and forenames != names['Forenames']:\n",
    "            names['Alternative Names'].append(forenames)\n",
    "    names['Alternative Names'] = ', '.join(names['Alternative Names'])\n",
    "\n",
    "    return names['Alternative Names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4ef380",
   "metadata": {
    "id": "ba4ef380"
   },
   "outputs": [],
   "source": [
    "def process(xmlfile):\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    data_dict = {}\n",
    "    def extract_text(element):\n",
    "        return element.text if element is not None else \"N/A\"\n",
    "    creator_group = root.find(\".//creatorGroup\")\n",
    "    if creator_group is not None:\n",
    "        data_dict[\"Author_Forenames\"] = extract_text(creator_group.find(\"creator/forenames\"))\n",
    "        data_dict[\"Author_Surname\"] = extract_text(creator_group.find(\"creator/surname\"))\n",
    "    else:\n",
    "        data_dict[\"Author_Forenames\"] = \"N/A\"\n",
    "        data_dict[\"Author_Surname\"] = \"N/A\"\n",
    "    parents = root.findall(\".//metaItem[@about='parents']/metaItem[@about='parent']\")\n",
    "    parent_data = []\n",
    "    for parent in parents:\n",
    "        parent_info = {}\n",
    "        parent_info[\"Gender\"] = extract_text(parent.find(\".//metaDescribes[@predicate='gender']\"))\n",
    "        parent_info[\"MainName\"] = extract_text(parent.find(\".//metaDescribes[@predicate='mainName']\"))\n",
    "        parent_info[\"ForeNames\"] = extract_text(parent.find(\".//metaDescribes[@predicate='foreNames']\"))\n",
    "        parent_info[\"Birth\"] = extract_text(parent.find(\".//metaItem[@about='birth']/metaDescribes[@predicate='date']\"))\n",
    "        parent_info[\"Death\"] = extract_text(parent.find(\".//metaItem[@about='death']/metaDescribes[@predicate='date']\"))\n",
    "        parent_info[\"Occupation\"] = extract_text(parent.find(\".//metaDescribes[@predicate='occupation']\"))\n",
    "        parent_data.append(parent_info)\n",
    "    profile = root.find(\".//metaItem[@about='profile']\")\n",
    "    if profile is not None:\n",
    "        data_dict[\"Gender\"] = extract_text(profile.find(\".//metaDescribes[@predicate='gender']\"))\n",
    "        data_dict[\"MainName\"] = extract_text(profile.find(\".//metaDescribes[@predicate='mainName']\"))\n",
    "        data_dict[\"foreNames\"] = extract_text(profile.find(\".//metaDescribes[@predicate='foreNames']\"))\n",
    "        data_dict[\"ShortForm\"] = extract_text(profile.find(\".//metaDescribes[@predicate='ShortForm']\"))\n",
    "        data_dict[\"Birth\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaDescribes[@predicate='date']\"))\n",
    "        data_dict[\"Birth Place - Street\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaItem/metaDescribes[@predicate='street']\"))\n",
    "        data_dict[\"Birth Place - Town\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaItem/metaDescribes[@predicate='town']\"))\n",
    "        data_dict[\"Birth Place - City\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaItem/metaDescribes[@predicate='city']\"))\n",
    "        data_dict[\"Birth Place - County\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaItem/metaDescribes[@predicate='county']\"))\n",
    "        data_dict[\"Birth Place - Country\"] = extract_text(profile.find(\".//metaItem[@about='birth']/metaItem/metaDescribes[@predicate='country']\"))\n",
    "        data_dict[\"Death\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaDescribes[@predicate='date']\"))\n",
    "        data_dict[\"Death Place - Street\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaItem/metaDescribes[@predicate='street']\"))\n",
    "        data_dict[\"Death Place - Town\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaItem/metaDescribes[@predicate='town']\"))\n",
    "        data_dict[\"Death Place - City\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaItem/metaDescribes[@predicate='city']\"))\n",
    "        data_dict[\"Death Place - County\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaItem/metaDescribes[@predicate='county']\"))\n",
    "        data_dict[\"Death Place - Country\"] = extract_text(profile.find(\".//metaItem[@about='death']/metaItem/metaDescribes[@predicate='country']\"))\n",
    "\n",
    "        occupations = extract_person_occupations(xmlfile)\n",
    "        data_dict[\"Occupation\"] = ', '.join(occupations) if occupations else 'N/A'\n",
    "\n",
    "        alternative_names = extract_names(xmlfile, print_names=True)\n",
    "        data_dict[\"Alternative Names\"] = alternative_names\n",
    "\n",
    "    else:\n",
    "        data_dict[\"Gender\"] = \"N/A\"\n",
    "        data_dict[\"MainName\"] = \"N/A\"\n",
    "        data_dict[\"foreNames\"] = \"N/A\"\n",
    "        data_dict[\"ShortForm\"] = \"N/A\"\n",
    "        data_dict[\"Birth\"] = \"N/A\"\n",
    "        data_dict[\"Birth Place - Street\"] = \"N/A\"\n",
    "        data_dict[\"Birth Place - City\"] = \"N/A\"\n",
    "        data_dict[\"Birth Place - Country\"] = \"N/A\"\n",
    "        data_dict[\"Birth Place - Town\"] = \"N/A\"\n",
    "        data_dict[\"Birth Place - County\"] = \"N/A\"\n",
    "        data_dict[\"Death\"] = \"N/A\"\n",
    "        data_dict[\"Death Place - Street\"] = \"N/A\"\n",
    "        data_dict[\"Death Place - Town\"] = \"N/A\"\n",
    "        data_dict[\"Death Place - County\"] = \"N/A\"\n",
    "        data_dict[\"Death Place - City\"] = \"N/A\"\n",
    "        data_dict[\"Death Place - Country\"] = \"N/A\"\n",
    "        data_dict[\"Occupation\"] = \"N/A\"\n",
    "        data_dict[\"Alternative Names\"] = \"N/A\"\n",
    "\n",
    "    # Add parent data to the dictionary\n",
    "    for i, parent_info in enumerate(parent_data, start=1):\n",
    "        for key, value in parent_info.items():\n",
    "            if i == 1:\n",
    "                data_dict[f\"Father_{key}\"] = value\n",
    "            elif i == 2:\n",
    "                data_dict[f\"Mother_{key}\"] = value\n",
    "\n",
    "    # Initialize data dictionary with empty school and college columns\n",
    "    school_columns = []\n",
    "    college_columns = []\n",
    "\n",
    "    # Extract education data\n",
    "    education_data = root.findall(\".//metaItem[@about='education']\")\n",
    "    for idx, education_item in enumerate(education_data):\n",
    "        school = education_item.find(\".//metaItem[@about='school']\")\n",
    "        if school is not None:\n",
    "            school_name = extract_text(school.find(\".//metaDescribes[@predicate='institution']\"))\n",
    "            school_city = extract_text(school.find(\".//metaDescribes[@predicate='city']\"))\n",
    "            school_country = extract_text(school.find(\".//metaDescribes[@predicate='country']\"))\n",
    "            data_dict[f\"School_{idx+1}\"] = school_name\n",
    "            data_dict[f\"School_{idx+1}_City\"] = school_city\n",
    "            data_dict[f\"School_{idx+1}_Country\"] = school_country\n",
    "\n",
    "        college = education_item.find(\".//metaItem[@about='college']\")\n",
    "        if college is not None:\n",
    "            college_name = extract_text(college.find(\".//metaDescribes[@predicate='institution']\"))\n",
    "            college_city = extract_text(college.find(\".//metaDescribes[@predicate='town']\"))\n",
    "            college_country = extract_text(college.find(\".//metaDescribes[@predicate='country']\"))\n",
    "            data_dict[f\"College_{idx+1}\"] = college_name\n",
    "            data_dict[f\"College_{idx+1}_City\"] = college_city\n",
    "            data_dict[f\"College_{idx+1}_Country\"] = college_country\n",
    "\n",
    "    # Extract residence data\n",
    "    residence = root.find(\".//metaItem[@about='residence']\")\n",
    "    if residence is not None:\n",
    "        data_dict[\"Residence_Date\"] = extract_text(residence.find(\".//metaDescribes[@predicate='date']\"))\n",
    "        data_dict[\"Residence_Town\"] = extract_text(residence.find(\".//metaItem/metaDescribes[@predicate='town']\"))\n",
    "        data_dict[\"Residence_Street\"] = extract_text(residence.find(\".//metaItem/metaDescribes[@predicate='street']\"))\n",
    "        data_dict[\"Residence_City\"] = extract_text(residence.find(\".//metaItem/metaDescribes[@predicate='city']\"))\n",
    "        data_dict[\"Residence_County\"] = extract_text(residence.find(\".//metaItem/metaDescribes[@predicate='county']\"))\n",
    "        data_dict[\"Residence_Country\"] = extract_text(residence.find(\".//metaItem/metaDescribes[@predicate='country']\"))\n",
    "    else:\n",
    "        data_dict[\"Residence_Date\"] = \"N/A\"\n",
    "        data_dict[\"Residence_Town\"] = \"N/A\"\n",
    "        data_dict[\"Residence_Street\"] = 'N/A'\n",
    "        data_dict[\"Residence_City\"] = 'N/A'\n",
    "        data_dict[\"Residence_County\"] = \"N/A\"\n",
    "        data_dict[\"Residence_Country\"] = \"N/A\"\n",
    "\n",
    "    # Initialize an empty list to store taxonomy terms\n",
    "    taxonomy_terms_list = []\n",
    "\n",
    "    # Extract taxonomy data\n",
    "    taxonomy_data = root.findall(\".//metaItem[@about='taxonomyTerm']\")\n",
    "    for taxonomy_term in taxonomy_data:\n",
    "        taxonomy = extract_text(taxonomy_term.find(\".//metaDescribes[@predicate='taxonomy']\"))\n",
    "        code = extract_text(taxonomy_term.find(\".//metaDescribes[@predicate='code']\"))\n",
    "        # Combine taxonomy and code into a single string\n",
    "        combined_taxonomy = f\"{taxonomy} ({code})\"\n",
    "        taxonomy_terms_list.append(combined_taxonomy)\n",
    "\n",
    "    # Join the taxonomy terms into a single string separated by commas\n",
    "    data_dict[\"Taxonomy_Terms\"] = ', '.join(taxonomy_terms_list)\n",
    "\n",
    "    # Initialize an empty list to store institution names\n",
    "    institution_names = []\n",
    "\n",
    "    # Extract institution data\n",
    "    institution_data = root.findall(\".//place[@institution]\")\n",
    "    for institution_item in institution_data:\n",
    "        institution_name = institution_item.get(\"institution\")\n",
    "        if institution_name and institution_name not in institution_names:\n",
    "            institution_names.append(institution_name)\n",
    "\n",
    "    # Join the institution names into a single string separated by commas\n",
    "    data_dict[\"Institutions\"] = ', '.join(institution_names)\n",
    "\n",
    "    # Flag to indicate whether data was successfully extracted using format 1\n",
    "    format_1_success = False\n",
    "\n",
    "    # Extract wealth at death data using format 1\n",
    "    wealth_data = root.find(\".//section[@role='wealthAtDeath']\")\n",
    "    if wealth_data is not None:\n",
    "        wealth_amount = extract_text(wealth_data.find(\".//span[@role='money']\"))\n",
    "        wealth_probate_date = extract_text(wealth_data.find(\".//p\"))\n",
    "        wealth_abbrev = extract_text(wealth_data.find(\".//abbrev\"))\n",
    "\n",
    "        # If data is successfully extracted using format 1, set the flag\n",
    "        if wealth_amount != \"N/A\" and wealth_probate_date != \"N/A\":\n",
    "            format_1_success = True\n",
    "\n",
    "    # If data is not successfully extracted using format 1, try format 2\n",
    "    if not format_1_success:\n",
    "        # Extract wealth at death data using format 2\n",
    "        wealth_data = root.find(\".//section[@role='wealthAtDeath']\")\n",
    "        if wealth_data is not None:\n",
    "            # Check if the new format is present\n",
    "            div1 = wealth_data.find(\".//div1\")\n",
    "            if div1 is not None:\n",
    "                # Extract wealth amount from the new format\n",
    "                wealth_p_tag = div1.find(\"./p\")\n",
    "                if wealth_p_tag is not None and wealth_p_tag.text is not None:\n",
    "                    wealth_text = wealth_p_tag.text\n",
    "                    wealth_amount_match = re.search(r'£([\\d,]+)', wealth_text)\n",
    "                    if wealth_amount_match:\n",
    "                        wealth_amount = wealth_amount_match.group(1).replace(',', '')\n",
    "                    else:\n",
    "                        wealth_amount = \"N/A\"\n",
    "                else:\n",
    "                    wealth_amount = \"N/A\"\n",
    "            else:\n",
    "                # Extract wealth amount from the original format\n",
    "                wealth_amount = extract_text(wealth_data.find(\".//span[@role='money']\"))\n",
    "            # Extract other relevant data (if applicable)\n",
    "            wealth_probate_date = extract_text(wealth_data.find(\".//p\"))\n",
    "            wealth_abbrev = extract_text(wealth_data.find(\".//abbrev\"))\n",
    "\n",
    "        else:\n",
    "            wealth_amount = \"N/A\"\n",
    "            wealth_probate_date = \"N/A\"\n",
    "            wealth_abbrev = \"N/A\"\n",
    "\n",
    "    # Add wealth at death data to the dictionary\n",
    "    data_dict[\"Wealth_Amount\"] = wealth_amount\n",
    "    data_dict[\"Wealth_Probate_Date\"] = wealth_probate_date\n",
    "    data_dict[\"Wealth_Abbreviation\"] = wealth_abbrev\n",
    "\n",
    "\n",
    "    # Extract spouse data\n",
    "    spouses = root.findall(\".//metaItem[@about='spouses']/metaItem[@about='spouse']\")\n",
    "    spouse_data = []\n",
    "    for spouse in spouses:\n",
    "        spouse_info = {}\n",
    "        spouse_info[\"Gender\"] = extract_text(spouse.find(\".//metaDescribes[@predicate='gender']\"))\n",
    "        spouse_info[\"MainName\"] = extract_text(spouse.find(\".//metaDescribes[@predicate='mainName']\"))\n",
    "        spouse_info[\"ForeNames\"] = extract_text(spouse.find(\".//metaDescribes[@predicate='foreNames']\"))\n",
    "        spouse_info[\"Birth\"] = extract_text(spouse.find(\".//metaItem[@about='birth']/metaDescribes[@predicate='date']\"))\n",
    "        spouse_info[\"Death\"] = extract_text(spouse.find(\".//metaItem[@about='death']/metaDescribes[@predicate='date']\"))\n",
    "        spouse_info[\"Occupation\"] = extract_text(spouse.find(\".//metaDescribes[@predicate='occupation']\"))\n",
    "        spouse_info[\"Relationship_Date\"] = extract_text(spouse.find(\".//metaItem[@about='relationship_dates']/metaDescribes[@predicate='date']\"))\n",
    "        spouse_data.append(spouse_info)\n",
    "\n",
    "    # Add spouse data to the dictionary\n",
    "    for i, spouse_info in enumerate(spouse_data, start=1):\n",
    "        for key, value in spouse_info.items():\n",
    "            data_dict[f\"Spouse_{i}_{key}\"] = value\n",
    "\n",
    "    # Initialize spouse columns in DataFrame\n",
    "    spouse_columns = []\n",
    "    if spouse_data:\n",
    "        spouse_columns = [f\"Spouse_{i}_{key}\" for key in spouse_data[0].keys() for i in range(1, len(spouse_data)+1)]\n",
    "\n",
    "    # Update spouse columns in DataFrame\n",
    "    for col in spouse_columns:\n",
    "        if col not in data_dict:\n",
    "            data_dict[col] = \"N/A\"\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffcfd70d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffcfd70d",
    "outputId": "b696f1e1-3bc2-4141-ae05-a5bf4e8be1b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 39518/39518 [01:40<00:00, 392.92it/s]\n"
     ]
    }
   ],
   "source": [
    "def main(directory):\n",
    "    log_file_path = os.path.join(directory, 'log.txt')\n",
    "    all_data = []\n",
    "    if not os.path.exists(log_file_path):\n",
    "        open(log_file_path, 'w').close()\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        processed_files = set(file.read().splitlines())\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        if filename.endswith('.xml') and filename not in processed_files:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            file_data = process(file_path)\n",
    "            all_data.append(file_data)\n",
    "            with open(log_file_path, 'a') as log_file:\n",
    "                log_file.write(filename + '\\n')\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_excel(\"dataset_new.xlsx\", index=False)\n",
    "if __name__ == \"__main__\":\n",
    "    directory = r'C:\\Users\\Muneeb Nabeel\\Desktop\\FINAL desertation code files\\combined_xml'\n",
    "    main(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e63812",
   "metadata": {
    "id": "48e63812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39517 entries, 0 to 39516\n",
      "Columns: 136 entries, Author_Forenames to Spouse_12_Relationship_Date\n",
      "dtypes: float64(30), object(106)\n",
      "memory usage: 41.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('dataset_new.xlsx')\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80d116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: pyodbc in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (4.0.34)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from sqlalchemy) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\muneeb nabeel\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926816ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL Server', 'Microsoft Access Driver (*.mdb, *.accdb)', 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)', 'Microsoft Access Text Driver (*.txt, *.csv)', 'SQL Server Native Client RDA 11.0', 'ODBC Driver 17 for SQL Server']\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820898e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        r'SERVER=MUNEEB_LENOVO;'  # Replace with your server\n",
    "        r'DATABASE=Desertation;'  # Replace with your database\n",
    "        r'Trusted_Connection=yes;'\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ef3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('mssql+pyodbc://@MUNEEB_LENOVO/Desertation?driver=ODBC Driver 17 for SQL Server;trusted_connection=yes')\n",
    "# df = pd.read_sql_query(\"SELECT * FROM your_table\", engine)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d73c8856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ODBC drivers:\n",
      "SQL Server\n",
      "Microsoft Access Driver (*.mdb, *.accdb)\n",
      "Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)\n",
      "Microsoft Access Text Driver (*.txt, *.csv)\n",
      "SQL Server Native Client RDA 11.0\n",
      "ODBC Driver 17 for SQL Server\n",
      "ODBC Driver 18 for SQL Server\n",
      "\n",
      "\n",
      "Direct pyodbc connection successful.\n",
      "\n",
      "\n",
      "Query executed successfully: (1, )\n",
      "\n",
      "\n",
      "Failed to connect or query using SQLAlchemy: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n",
      "(Background on this error at: https://sqlalche.me/e/14/rvf5)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muneeb Nabeel\\AppData\\Local\\Temp\\ipykernel_10048\\1999813649.py:45: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections\n",
      "  engine = create_engine(connection_string, echo=True)\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def check_drivers():\n",
    "    print(\"Available ODBC drivers:\")\n",
    "    for driver in pyodbc.drivers():\n",
    "        print(driver)\n",
    "    print(\"\\n\")\n",
    "\n",
    "def test_pyodbc_connection(server, database):\n",
    "    try:\n",
    "        connection_string = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={server};\"\n",
    "            f\"DATABASE={database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        print(\"Direct pyodbc connection successful.\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"Failed to connect using pyodbc:\", str(e))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def test_pyodbc_query(server, database):\n",
    "    try:\n",
    "        connection_string = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={server};\"\n",
    "            f\"DATABASE={database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT 1\")  # Simple query\n",
    "        print(\"Query executed successfully:\", cursor.fetchone())\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(\"Failed to execute query using pyodbc:\", str(e))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def test_sqlalchemy_connection(connection_string):\n",
    "    try:\n",
    "        engine = create_engine(connection_string, echo=True)\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\"SELECT 1\")\n",
    "            print(\"SQLAlchemy connection successful. Query result:\", result.fetchone())\n",
    "    except Exception as e:\n",
    "        print(\"Failed to connect or query using SQLAlchemy:\", str(e))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    server = 'MUNEEB_LENOVO'\n",
    "    database = 'Desertation'\n",
    "\n",
    "    # Check available drivers\n",
    "    check_drivers()\n",
    "\n",
    "    # Test connection using pyodbc\n",
    "    test_pyodbc_connection(server, database)\n",
    "\n",
    "    # Test query execution using pyodbc\n",
    "    test_pyodbc_query(server, database)\n",
    "\n",
    "    # Build connection string for SQLAlchemy\n",
    "    connection_string = f'mssql+pyodbc://@{server}/{database}?trusted_connection=yes;driver=ODBC Driver 17 for SQL Server'\n",
    "\n",
    "    # Test connection using SQLAlchemy\n",
    "    test_sqlalchemy_connection(connection_string)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "432ffbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "connection_string = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\"\n",
    "conn = pyodbc.connect(connection_string)\n",
    "print(\"Connection successful!\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c70da841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muneeb Nabeel\\AppData\\Local\\Temp\\ipykernel_10048\\2984008719.py:2: SAWarning: No driver name specified; this is expected by PyODBC when using DSN-less connections\n",
      "  engine = create_engine(f'mssql+pyodbc://{server}/{database}?trusted_connection=yes;driver=ODBC Driver 17 for SQL Server', echo=True)\n"
     ]
    },
    {
     "ename": "InterfaceError",
     "evalue": "(pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n(Background on this error at: https://sqlalche.me/e/14/rvf5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3280\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[0;32m   3281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mdbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:310\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ConnectionFairy\u001b[38;5;241m.\u001b[39m_checkout(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:868\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m--> 868\u001b[0m     fairy \u001b[38;5;241m=\u001b[39m _ConnectionRecord\u001b[38;5;241m.\u001b[39mcheckout(pool)\n\u001b[0;32m    870\u001b[0m     fairy\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m pool\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:476\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckout\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pool):\n\u001b[1;32m--> 476\u001b[0m     rec \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39m_do_get()\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:145\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[38;5;241m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:143\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:256\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ConnectionRecord(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:371\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connect()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:665\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    666\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[38;5;241m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:661\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39m_invoke_creator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    662\u001b[0m pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:590\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:597\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams):\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "\u001b[1;31mInterfaceError\u001b[0m: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[0;32m      2\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmssql+pyodbc://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?trusted_connection=yes;driver=ODBC Driver 17 for SQL Server\u001b[39m\u001b[38;5;124m'\u001b[39m, echo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m      4\u001b[0m     result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mfetchone())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3234\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self, close_with_result)\u001b[0m\n\u001b[0;32m   3219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, close_with_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \n\u001b[0;32m   3222\u001b[0m \u001b[38;5;124;03m    The :class:`_engine.Connection` object is a facade that uses a DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3231\u001b[0m \n\u001b[0;32m   3232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_cls(\u001b[38;5;28mself\u001b[39m, close_with_result\u001b[38;5;241m=\u001b[39mclose_with_result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:96\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events, _allow_revalidate)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;241m=\u001b[39m _branch_from\u001b[38;5;241m.\u001b[39m_has_events\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbapi_connection \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         connection\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mraw_connection()\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_transaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__savepoint_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3313\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self, _connection)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m, _connection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m \n\u001b[0;32m   3294\u001b[0m \u001b[38;5;124;03m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3311\u001b[0m \n\u001b[0;32m   3312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_pool_connect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mconnect, _connection)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3283\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mdbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3283\u001b[0m         Connection\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m   3284\u001b[0m             e, dialect, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   3285\u001b[0m         )\n\u001b[0;32m   3286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3287\u001b[0m         util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m   3288\u001b[0m             sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   3289\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2117\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(newraise, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 2117\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m   2118\u001b[0m         sqlalchemy_exception, with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m], from_\u001b[38;5;241m=\u001b[39me\n\u001b[0;32m   2119\u001b[0m     )\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2121\u001b[0m     util\u001b[38;5;241m.\u001b[39mraise_(exc_info[\u001b[38;5;241m1\u001b[39m], with_traceback\u001b[38;5;241m=\u001b[39mexc_info[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3280\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3278\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[0;32m   3281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mdbapi\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:310\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ConnectionFairy\u001b[38;5;241m.\u001b[39m_checkout(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:868\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_checkout\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pool, threadconns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fairy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fairy:\n\u001b[1;32m--> 868\u001b[0m         fairy \u001b[38;5;241m=\u001b[39m _ConnectionRecord\u001b[38;5;241m.\u001b[39mcheckout(pool)\n\u001b[0;32m    870\u001b[0m         fairy\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[0;32m    871\u001b[0m         fairy\u001b[38;5;241m.\u001b[39m_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:476\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckout\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pool):\n\u001b[1;32m--> 476\u001b[0m     rec \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39m_do_get()\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         dbapi_connection \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mget_connection()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:145\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dec_overflow()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[38;5;241m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mpy3k \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;66;03m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:143\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inc_overflow():\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:256\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    254\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ConnectionRecord(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:371\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pool \u001b[38;5;241m=\u001b[39m pool\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connect:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__connect()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalize_callback \u001b[38;5;241m=\u001b[39m deque()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:665\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[0;32m    666\u001b[0m         pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError on connect(): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[38;5;241m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[38;5;241m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mpy3k \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;66;03m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:661\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi_connection \u001b[38;5;241m=\u001b[39m connection \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39m_invoke_creator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    662\u001b[0m     pool\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new connection \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, connection)\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:590\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m connection\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dialect\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:597\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams):\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbapi\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39mcargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcparams)\n",
      "\u001b[1;31mInterfaceError\u001b[0m: (pyodbc.InterfaceError) ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')\n(Background on this error at: https://sqlalche.me/e/14/rvf5)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(f'mssql+pyodbc://{server}/{database}?trusted_connection=yes;driver=ODBC Driver 17 for SQL Server', echo=True)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(\"SELECT 1\")\n",
    "    print(\"Query result:\", result.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d60af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available ODBC Drivers: ['SQL Server', 'Microsoft Access Driver (*.mdb, *.accdb)', 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)', 'Microsoft Access Text Driver (*.txt, *.csv)', 'SQL Server Native Client RDA 11.0', 'ODBC Driver 17 for SQL Server', 'ODBC Driver 18 for SQL Server']\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "print(\"Available ODBC Drivers:\", pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e418f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "connection_string = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=MUNEEB_LENOVO;\"\n",
    "    \"DATABASE=Desertation;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "try:\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    print(\"Connection successful!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect using pyodbc:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7365e362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 17:17:04,080 INFO sqlalchemy.engine.Engine SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)\n",
      "2024-04-14 17:17:04,080 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-04-14 17:17:04,080 INFO sqlalchemy.engine.Engine SELECT schema_name()\n",
      "2024-04-14 17:17:04,090 INFO sqlalchemy.engine.Engine [generated in 0.00070s] ()\n",
      "2024-04-14 17:17:04,100 INFO sqlalchemy.engine.Engine SELECT CAST('test max support' AS NVARCHAR(max))\n",
      "2024-04-14 17:17:04,100 INFO sqlalchemy.engine.Engine [generated in 0.00098s] ()\n",
      "2024-04-14 17:17:04,100 INFO sqlalchemy.engine.Engine SELECT 1\n",
      "2024-04-14 17:17:04,100 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "Query result: (1,)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "connection_url = URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    host=server,\n",
    "    database=database,\n",
    "    query={\n",
    "        \"trusted_connection\": \"yes\",\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\"\n",
    "    }\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_url, echo=True)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(\"SELECT 1\")\n",
    "        print(\"Query result:\", result.fetchone())\n",
    "except Exception as e:\n",
    "    print(\"SQLAlchemy connection error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe4fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 17:22:34,113 INFO sqlalchemy.engine.Engine SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)\n",
      "2024-04-14 17:22:34,113 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2024-04-14 17:22:34,117 INFO sqlalchemy.engine.Engine SELECT schema_name()\n",
      "2024-04-14 17:22:34,117 INFO sqlalchemy.engine.Engine [generated in 0.00094s] ()\n",
      "2024-04-14 17:22:34,134 INFO sqlalchemy.engine.Engine SELECT CAST('test max support' AS NVARCHAR(max))\n",
      "2024-04-14 17:22:34,134 INFO sqlalchemy.engine.Engine [generated in 0.00078s] ()\n",
      "2024-04-14 17:22:34,134 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-04-14 17:22:34,337 INFO sqlalchemy.engine.Engine SELECT [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME] \n",
      "FROM [INFORMATION_SCHEMA].[TABLES] \n",
      "WHERE [INFORMATION_SCHEMA].[TABLES].[TABLE_TYPE] = CAST(? AS NVARCHAR(max)) AND [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME] = CAST(? AS NVARCHAR(max)) AND [INFORMATION_SCHEMA].[TABLES].[TABLE_SCHEMA] = CAST(? AS NVARCHAR(max))\n",
      "2024-04-14 17:22:34,340 INFO sqlalchemy.engine.Engine [generated in 0.00175s] ('BASE TABLE', 'MyRawDataTable', 'dbo')\n",
      "2024-04-14 17:22:34,539 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE [MyRawDataTable] (\n",
      "\t[Author_Forenames] VARCHAR(max) NULL, \n",
      "\t[Author_Surname] VARCHAR(max) NULL, \n",
      "\t[Gender] VARCHAR(max) NULL, \n",
      "\t[MainName] VARCHAR(max) NULL, \n",
      "\t[foreNames] VARCHAR(max) NULL, \n",
      "\t[ShortForm] VARCHAR(max) NULL, \n",
      "\t[Birth] VARCHAR(max) NULL, \n",
      "\t[Birth Place - Street] VARCHAR(max) NULL, \n",
      "\t[Birth Place - Town] VARCHAR(max) NULL, \n",
      "\t[Birth Place - City] VARCHAR(max) NULL, \n",
      "\t[Birth Place - County] VARCHAR(max) NULL, \n",
      "\t[Birth Place - Country] VARCHAR(max) NULL, \n",
      "\t[Death] VARCHAR(max) NULL, \n",
      "\t[Death Place - Street] VARCHAR(max) NULL, \n",
      "\t[Death Place - Town] VARCHAR(max) NULL, \n",
      "\t[Death Place - City] VARCHAR(max) NULL, \n",
      "\t[Death Place - County] VARCHAR(max) NULL, \n",
      "\t[Death Place - Country] VARCHAR(max) NULL, \n",
      "\t[Occupation] VARCHAR(max) NULL, \n",
      "\t[Alternative Names] VARCHAR(max) NULL, \n",
      "\t[Father_Gender] VARCHAR(max) NULL, \n",
      "\t[Father_MainName] VARCHAR(max) NULL, \n",
      "\t[Father_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Father_Birth] VARCHAR(max) NULL, \n",
      "\t[Father_Death] VARCHAR(max) NULL, \n",
      "\t[Father_Occupation] VARCHAR(max) NULL, \n",
      "\t[Mother_Gender] VARCHAR(max) NULL, \n",
      "\t[Mother_MainName] VARCHAR(max) NULL, \n",
      "\t[Mother_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Mother_Birth] VARCHAR(max) NULL, \n",
      "\t[Mother_Death] VARCHAR(max) NULL, \n",
      "\t[Mother_Occupation] VARCHAR(max) NULL, \n",
      "\t[Residence_Date] VARCHAR(max) NULL, \n",
      "\t[Residence_Town] VARCHAR(max) NULL, \n",
      "\t[Residence_Street] VARCHAR(max) NULL, \n",
      "\t[Residence_City] VARCHAR(max) NULL, \n",
      "\t[Residence_County] VARCHAR(max) NULL, \n",
      "\t[Residence_Country] VARCHAR(max) NULL, \n",
      "\t[Taxonomy_Terms] VARCHAR(max) NULL, \n",
      "\t[Institutions] VARCHAR(max) NULL, \n",
      "\t[Wealth_Amount] VARCHAR(max) NULL, \n",
      "\t[Wealth_Probate_Date] VARCHAR(max) NULL, \n",
      "\t[Wealth_Abbreviation] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_Birth] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_Death] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_1_Relationship_Date] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_Birth] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_Death] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_2_Relationship_Date] VARCHAR(max) NULL, \n",
      "\t[School_1] VARCHAR(max) NULL, \n",
      "\t[School_1_City] VARCHAR(max) NULL, \n",
      "\t[School_1_Country] VARCHAR(max) NULL, \n",
      "\t[College_1] VARCHAR(max) NULL, \n",
      "\t[College_1_City] VARCHAR(max) NULL, \n",
      "\t[College_1_Country] VARCHAR(max) NULL, \n",
      "\t[Spouse_3_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_3_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_3_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_3_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_3_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_3_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_3_Relationship_Date] VARCHAR(max) NULL, \n",
      "\t[Spouse_4_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_4_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_4_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_4_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_4_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_4_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_4_Relationship_Date] VARCHAR(max) NULL, \n",
      "\t[Spouse_5_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_5_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_5_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_5_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_5_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_5_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_5_Relationship_Date] VARCHAR(max) NULL, \n",
      "\t[Spouse_6_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_6_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_6_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_6_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_6_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_6_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_6_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[Spouse_7_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_7_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_7_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_7_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_7_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_7_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_7_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[Spouse_8_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_8_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_8_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_8_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_8_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_8_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_8_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[School_2] VARCHAR(max) NULL, \n",
      "\t[School_2_City] VARCHAR(max) NULL, \n",
      "\t[School_2_Country] VARCHAR(max) NULL, \n",
      "\t[Spouse_9_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_9_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_9_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_9_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_9_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_9_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_9_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[Spouse_10_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_10_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_10_ForeNames] FLOAT(53) NULL, \n",
      "\t[Spouse_10_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_10_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_10_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_10_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[Spouse_11_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_11_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_11_ForeNames] VARCHAR(max) NULL, \n",
      "\t[Spouse_11_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_11_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_11_Occupation] VARCHAR(max) NULL, \n",
      "\t[Spouse_11_Relationship_Date] FLOAT(53) NULL, \n",
      "\t[Spouse_12_Gender] VARCHAR(max) NULL, \n",
      "\t[Spouse_12_MainName] VARCHAR(max) NULL, \n",
      "\t[Spouse_12_ForeNames] FLOAT(53) NULL, \n",
      "\t[Spouse_12_Birth] FLOAT(53) NULL, \n",
      "\t[Spouse_12_Death] FLOAT(53) NULL, \n",
      "\t[Spouse_12_Occupation] FLOAT(53) NULL, \n",
      "\t[Spouse_12_Relationship_Date] FLOAT(53) NULL\n",
      ")\n",
      "\n",
      "\n",
      "2024-04-14 17:22:34,539 INFO sqlalchemy.engine.Engine [no key 0.00093s] ()\n",
      "2024-04-14 17:22:38,170 INFO sqlalchemy.engine.Engine INSERT INTO [MyRawDataTable] ([Author_Forenames], [Author_Surname], [Gender], [MainName], [foreNames], [ShortForm], [Birth], [Birth Place - Street], [Birth Place - Town], [Birth Place - City], [Birth Place - County], [Birth Place - Country], [Death], [Death Place - Street], [Death Place - Town], [Death Place - City], [Death Place - County], [Death Place - Country], [Occupation], [Alternative Names], [Father_Gender], [Father_MainName], [Father_ForeNames], [Father_Birth], [Father_Death], [Father_Occupation], [Mother_Gender], [Mother_MainName], [Mother_ForeNames], [Mother_Birth], [Mother_Death], [Mother_Occupation], [Residence_Date], [Residence_Town], [Residence_Street], [Residence_City], [Residence_County], [Residence_Country], [Taxonomy_Terms], [Institutions], [Wealth_Amount], [Wealth_Probate_Date], [Wealth_Abbreviation], [Spouse_1_Gender], [Spouse_1_MainName], [Spouse_1_ForeNames], [Spouse_1_Birth], [Spouse_1_Death], [Spouse_1_Occupation], [Spouse_1_Relationship_Date], [Spouse_2_Gender], [Spouse_2_MainName], [Spouse_2_ForeNames], [Spouse_2_Birth], [Spouse_2_Death], [Spouse_2_Occupation], [Spouse_2_Relationship_Date], [School_1], [School_1_City], [School_1_Country], [College_1], [College_1_City], [College_1_Country], [Spouse_3_Gender], [Spouse_3_MainName], [Spouse_3_ForeNames], [Spouse_3_Birth], [Spouse_3_Death], [Spouse_3_Occupation], [Spouse_3_Relationship_Date], [Spouse_4_Gender], [Spouse_4_MainName], [Spouse_4_ForeNames], [Spouse_4_Birth], [Spouse_4_Death], [Spouse_4_Occupation], [Spouse_4_Relationship_Date], [Spouse_5_Gender], [Spouse_5_MainName], [Spouse_5_ForeNames], [Spouse_5_Birth], [Spouse_5_Death], [Spouse_5_Occupation], [Spouse_5_Relationship_Date], [Spouse_6_Gender], [Spouse_6_MainName], [Spouse_6_ForeNames], [Spouse_6_Birth], [Spouse_6_Death], [Spouse_6_Occupation], [Spouse_6_Relationship_Date], [Spouse_7_Gender], [Spouse_7_MainName], [Spouse_7_ForeNames], [Spouse_7_Birth], [Spouse_7_Death], [Spouse_7_Occupation], [Spouse_7_Relationship_Date], [Spouse_8_Gender], [Spouse_8_MainName], [Spouse_8_ForeNames], [Spouse_8_Birth], [Spouse_8_Death], [Spouse_8_Occupation], [Spouse_8_Relationship_Date], [School_2], [School_2_City], [School_2_Country], [Spouse_9_Gender], [Spouse_9_MainName], [Spouse_9_ForeNames], [Spouse_9_Birth], [Spouse_9_Death], [Spouse_9_Occupation], [Spouse_9_Relationship_Date], [Spouse_10_Gender], [Spouse_10_MainName], [Spouse_10_ForeNames], [Spouse_10_Birth], [Spouse_10_Death], [Spouse_10_Occupation], [Spouse_10_Relationship_Date], [Spouse_11_Gender], [Spouse_11_MainName], [Spouse_11_ForeNames], [Spouse_11_Birth], [Spouse_11_Death], [Spouse_11_Occupation], [Spouse_11_Relationship_Date], [Spouse_12_Gender], [Spouse_12_MainName], [Spouse_12_ForeNames], [Spouse_12_Birth], [Spouse_12_Death], [Spouse_12_Occupation], [Spouse_12_Relationship_Date]) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 17:22:38,174 INFO sqlalchemy.engine.Engine [generated in 2.81563s] (('Veronica', 'Kelly', 'f', 'Brayton', 'Lily', 'Brayton, Elizabeth [Lily]', '1876-06-23', 'White House, Stony Lane', 'Hindley', None, 'Lancashire', 'England', '1953-04-30', 'Redcliffe, East Cliffe Road', 'Dawlish', None, 'Devon', 'England', 'actor and theatrical producer', 'Elizabeth, Asche, Watson, Chalmers, Douglas', 'm', 'Brayton', 'John Grindall', '1842', '1892', 'medical practitioner', 'f', 'Alexander', 'Margaret', None, None, None, '1953', 'Dawlish', 'Redcliffe, East Cliffe Road', None, 'Devon', 'England', 'OccupationsAndRealmsOfRenown (732)', None, '£49,336 15', 'Wealth at Death', None, 'm', 'Asche', 'Thomas Stange Heiss Oscar', '1871', '1936', None, '1898-06-22', 'm', 'Watson', 'Douglas Chalmers', '1870', '1946', 'physician', '1938-06-15', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('Maggie B.', 'Gale', 'f', 'Collier', 'Constance', None, '1878-01-22', '2 Argyll Villas, Grove Road', 'New Windsor', None, 'Berkshire', 'England', '1955-04-25', None, None, 'New York', None, 'United States', 'actress', 'Laura Constance, Hardie, Boyle, Julian, L’Estrange', 'm', 'Hardie', 'Auguste Cheetham', '1852', '1939', 'actor', 'f', 'Collier', 'Eliza Georgina', '1853', '1914', 'dancer', '1955', None, 'West 57th Street', 'New York', None, 'United States', 'OccupationsAndRealmsOfRenown (732), OccupationsAndRealmsOfRenown (764)', 'Theatre Royal, Hull, His Majesty’s Theatre, New Amsterdam Theatre, Savoy, Globe, American Academy of Dramatic Art, Lenox Hill Hospital, New York', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('Rob', 'Steen', 'm', 'Close', 'Dennis', None, '1931-02-24', '8 New Street', 'Rawdon', None, 'West Riding of Yorkshire', 'England', '2015-09-13', None, 'Baildon', None, 'West Yorkshire', 'England', 'cricketer and footballer', 'Brian', 'm', 'Close', 'Harry', None, None, \"dyers' labourer\", 'f', 'Barrett', 'Esther', None, None, None, '2015', None, None, 'Baildon', 'West Yorkshire', 'England', 'OccupationsAndRealmsOfRenown (1642), OccupationsAndRealmsOfRenown (1606)', 'Aireborough Grammar School, Lord’s, Old Trafford, Ottery St Mary, Devon, Oval, Yorkshire Academy, St Chad’s Church, Far Headingley, Leeds', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Aireborough Grammar School', 'Aireborough', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('Rosemary', 'Scott', 'm', 'Barham', 'Richard Harris', None, '1788-12-06', '61 Burgate Street', 'Canterbury', None, 'Kent', 'England', '1845-06-17', 'canonry house', \"St Paul's Churchyard\", None, 'London', 'England', 'writer and Church of England clergyman', None, 'm', 'Barham', 'Richard Harris', None, None, 'alderman', 'f', 'Fox', 'Elizabeth', None, None, 'housekeeper', None, 'Canterbury', '61 Burgate Street', None, 'Kent', 'England', 'OccupationsAndRealmsOfRenown (1128), OccupationsAndRealmsOfRenown (1893), OdnbOccupations (100100), OdnbOccupations (151204), ReligiousAffiliation (3006)', \"St Paul's School, Brasenose College, Oxford, St Paul's Cathedral, St Mary Magdalen and St Gregory by Paul, Garrick Club, Garrick, St Paul's, St Augustine with St Faith's, St Mary Magdalene and St Gregory, Kensal Green cemetery\", None, None, None, 'f', 'Smart', 'Caroline', None, None, None, None, None, None, None, None, None, None, None, \"St Paul's School\", None, 'England', 'Brasenose College', 'Oxford', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('R. K.', 'Webb', 'm', 'Barham', 'Thomas Foster', None, '1794-09-10', None, 'Hendon', None, 'Middlesex', 'England', '1869-03-03', 'Castle Dyke', 'near Newton Abbot', None, 'Devon', 'England', 'physician and classical scholar', None, 'm', 'Barham', 'Thomas Foster', None, None, 'musician and writer', 'f', 'Morton', 'Mary Ann', None, None, None, None, 'Penzance', None, None, 'Cornwall', 'England', 'OccupationsAndRealmsOfRenown (288), OccupationsAndRealmsOfRenown (815), OdnbOccupations (120601), OdnbOccupations (170601), ReligiousAffiliation (194004), ReligiousAffiliation (209), ReligiousAffiliation (3006)', \"Queens' College, Cambridge, Guy's, St Thomas's, Cambridge, Exeter Dispensary, West of England Institution for the Instruction and Employment of the Blind, George's Chapel\", '14000', 'Wealth at Death', None, 'f', 'Garratt', 'Sarah', None, None, None, None, 'f', 'Henryson', 'Margaret', None, None, None, None, None, None, None, \"Queens' College\", 'Cambridge', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('Thompson', 'Cooper', 'm', 'Barham', 'William Foster', None, '1802-10-22', None, 'Marazion', None, 'Cornwall', 'England', '1845-01-28', None, None, None, None, None, 'writer', None, 'm', 'Barham', 'Thomas Foster', None, None, 'writer', 'f', 'Morton', 'Mary Ann', None, None, None, None, None, None, None, 'Kent', 'England', 'OccupationsAndRealmsOfRenown (1893), OdnbOccupations (100100), ReligiousAffiliation (3006)', \"Queens' College, Cambridge, Trinity College, Cambridge, Trinity\", None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 'Bodmin grammar school', None, 'England', \"Queens' College\", 'Cambridge', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), ('K. D.', 'Reynolds', 'f', 'Baring', 'Harriet Mary', None, '1805-05-14', None, None, None, None, None, '1857-05-04', None, 'Paris', None, None, 'France', 'literary hostess', 'Louisa, Bingham', 'm', 'Montagu', 'George John', None, None, 'landowner and politician', 'f', 'Corry', 'Mary Anne Julia Louisa Harriet', None, None, None, None, None, 'Bath House, Piccadilly', None, 'London', 'England', 'OccupationsAndRealmsOfRenown (1872), OdnbOccupations (101002)', 'Oriel College, Oxford', '180000', 'Wealth at Death', None, 'm', 'Baring', 'William Bingham', None, None, 'politician', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), (None, None, 'm', 'Baring', 'William Bingham', None, '1799-06', None, 'Philadelphia', None, None, 'USA', '1864-03-23', 'The Grange', 'Alresford', None, 'Hampshire', 'England', None, 'Bingham', 'm', 'Baring', 'Alexander', None, None, None, 'f', 'Bingham', 'Ann Louisa', None, None, None, None, 'Alresford', 'The Grange', None, 'Hampshire', 'England', 'OccupationsAndRealmsOfRenown (1023), OdnbOccupations (140302), ReligiousAffiliation (3006)', None, None, None, None, 'f', 'Montagu', 'Harriet Mary', None, None, None, None, 'f', 'Mackenzie', 'Louisa Caroline', None, None, None, None, None, None, None, 'Oriel College', 'Oxford', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)  ... displaying 10 of 39517 total bound parameter sets ...  ('Ronald T.', 'Raines', 'm', 'Knowles', 'Jeremy Randall', None, '1935-04-28', '1 Murray Road', 'Rugby', None, 'Warwickshire', 'England', '2008-04-03', None, 'Cambridge', None, 'Massachusetts', 'USA', 'chemist', 'Carey, Janey', 'm', 'Knowles', 'Kenneth Guy Jack Charles', None, None, 'schoolmaster, economist, glass engraver', 'f', 'Swingler', 'Dorothy Helen', None, None, None, None, 'Cambridge', '67 Francis Avenue', None, 'Massachusetts', 'USA', 'OccupationsAndRealmsOfRenown (1421), OdnbOccupations (180602)', 'University College, Nottingham, Institute of Economics and Statistics, Magdalen College School, Oxford, Balliol College, Oxford, Oxford, Merton Colle ... (146 characters truncated) ... ward Hughes Medical Institute, Eidgenössische Technische Hochschule, Balliol, Wadham, University of Edinburgh, Mount Auburn cemetery, Memorial Church', None, None, None, 'f', 'Davis', 'Jane Sheldon', None, None, 'librarian, archivist', None, None, None, None, None, None, None, None, 'Magdalen College School', None, 'England', 'Balliol College', 'Oxford', 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None), (None, None, 'm', 'Laing', 'John Maurice', None, '1918-02-01', \"42 St James's Road\", 'Carlisle', None, 'Cumberland', 'England', '2008-02-22', None, None, None, 'London', 'England', 'industrialist', 'Maurice', 'm', 'Laing', 'John William', None, None, 'builder and civil engineering contractor', 'f', 'Harland', 'Beatrice', None, None, None, None, None, None, None, None, None, 'OccupationsAndRealmsOfRenown (643), OdnbOccupations (110100), ReligiousAffiliation (3006), ReligiousAffiliation (39001)', None, None, None, None, 'f', 'Richards', 'Hilda Violet', None, None, 'library assistant', None, None, None, None, None, None, None, None, 'St Lawrence College', None, 'England', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 17:27:05,534 INFO sqlalchemy.engine.Engine SELECT [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME] \n",
      "FROM [INFORMATION_SCHEMA].[TABLES] \n",
      "WHERE [INFORMATION_SCHEMA].[TABLES].[TABLE_SCHEMA] = CAST(? AS NVARCHAR(max)) AND [INFORMATION_SCHEMA].[TABLES].[TABLE_TYPE] = CAST(? AS NVARCHAR(max)) ORDER BY [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME]\n",
      "2024-04-14 17:27:05,534 INFO sqlalchemy.engine.Engine [generated in 0.00113s] ('dbo', 'BASE TABLE')\n",
      "2024-04-14 17:27:05,558 INFO sqlalchemy.engine.Engine COMMIT\n",
      "Data successfully written to SQL Server\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "# Load the dataset from Excel\n",
    "df = pd.read_excel('dataset_new.xlsx')\n",
    "\n",
    "# Database connection parameters\n",
    "server = 'MUNEEB_LENOVO'\n",
    "database = 'Desertation'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "# Using URL object to ensure proper formatting\n",
    "connection_url = URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"\",  # Not needed for Windows Authentication\n",
    "    password=\"\",  # Not needed for Windows Authentication\n",
    "    host=server,\n",
    "    database=database,\n",
    "    query={\n",
    "        \"trusted_connection\": \"yes\",\n",
    "        \"driver\": driver\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the SQLAlchemy engine with verbose SQL output\n",
    "engine = create_engine(connection_url, echo=True)\n",
    "\n",
    "# Write the DataFrame to SQL Server in the specified table 'MyRawDataTable'\n",
    "try:\n",
    "    df.to_sql('MyRawDataTable', con=engine, index=False, if_exists='replace')\n",
    "    print(\"Data successfully written to SQL Server\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to write data to SQL Server:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1ee24",
   "metadata": {},
   "source": [
    "# 2. Cleaning and Structuring Raw Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation: Category aggregation: Group similar categories together to simplify the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1e80b",
   "metadata": {},
   "source": [
    "## Spouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb24bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spouse Name Related Columns:\n",
      "Spouse_10_ForeNames\n",
      "Spouse_10_MainName\n",
      "Spouse_11_ForeNames\n",
      "Spouse_11_MainName\n",
      "Spouse_12_ForeNames\n",
      "Spouse_12_MainName\n",
      "Spouse_1_ForeNames\n",
      "Spouse_1_MainName\n",
      "Spouse_2_ForeNames\n",
      "Spouse_2_MainName\n",
      "Spouse_3_ForeNames\n",
      "Spouse_3_MainName\n",
      "Spouse_4_ForeNames\n",
      "Spouse_4_MainName\n",
      "Spouse_5_ForeNames\n",
      "Spouse_5_MainName\n",
      "Spouse_6_ForeNames\n",
      "Spouse_6_MainName\n",
      "Spouse_7_ForeNames\n",
      "Spouse_7_MainName\n",
      "Spouse_8_ForeNames\n",
      "Spouse_8_MainName\n",
      "Spouse_9_ForeNames\n",
      "Spouse_9_MainName\n",
      "\n",
      "Columns not related to names:\n",
      "Spouse_10_Birth\n",
      "Spouse_10_Death\n",
      "Spouse_10_Gender\n",
      "Spouse_10_Occupation\n",
      "Spouse_10_Relationship_Date\n",
      "Spouse_11_Birth\n",
      "Spouse_11_Death\n",
      "Spouse_11_Gender\n",
      "Spouse_11_Occupation\n",
      "Spouse_11_Relationship_Date\n",
      "Spouse_12_Birth\n",
      "Spouse_12_Death\n",
      "Spouse_12_Gender\n",
      "Spouse_12_Occupation\n",
      "Spouse_12_Relationship_Date\n",
      "Spouse_1_Birth\n",
      "Spouse_1_Death\n",
      "Spouse_1_Gender\n",
      "Spouse_1_Occupation\n",
      "Spouse_1_Relationship_Date\n",
      "Spouse_2_Birth\n",
      "Spouse_2_Death\n",
      "Spouse_2_Gender\n",
      "Spouse_2_Occupation\n",
      "Spouse_2_Relationship_Date\n",
      "Spouse_3_Birth\n",
      "Spouse_3_Death\n",
      "Spouse_3_Gender\n",
      "Spouse_3_Occupation\n",
      "Spouse_3_Relationship_Date\n",
      "Spouse_4_Birth\n",
      "Spouse_4_Death\n",
      "Spouse_4_Gender\n",
      "Spouse_4_Occupation\n",
      "Spouse_4_Relationship_Date\n",
      "Spouse_5_Birth\n",
      "Spouse_5_Death\n",
      "Spouse_5_Gender\n",
      "Spouse_5_Occupation\n",
      "Spouse_5_Relationship_Date\n",
      "Spouse_6_Birth\n",
      "Spouse_6_Death\n",
      "Spouse_6_Gender\n",
      "Spouse_6_Occupation\n",
      "Spouse_6_Relationship_Date\n",
      "Spouse_7_Birth\n",
      "Spouse_7_Death\n",
      "Spouse_7_Gender\n",
      "Spouse_7_Occupation\n",
      "Spouse_7_Relationship_Date\n",
      "Spouse_8_Birth\n",
      "Spouse_8_Death\n",
      "Spouse_8_Gender\n",
      "Spouse_8_Occupation\n",
      "Spouse_8_Relationship_Date\n",
      "Spouse_9_Birth\n",
      "Spouse_9_Death\n",
      "Spouse_9_Gender\n",
      "Spouse_9_Occupation\n",
      "Spouse_9_Relationship_Date\n"
     ]
    }
   ],
   "source": [
    "# Define a list to store spouse-related columns\n",
    "spouse_columns = []\n",
    "\n",
    "# Define a list to store all columns\n",
    "all_columns = []\n",
    "\n",
    "# Iterate through numbers from 1 to 12\n",
    "for i in range(1, 13):\n",
    "    # Construct column names using string formatting\n",
    "    gender_column = f\"Spouse_{i}_Gender\"\n",
    "    main_name_column = f\"Spouse_{i}_MainName\"\n",
    "    forenames_column = f\"Spouse_{i}_ForeNames\"\n",
    "    birth_column = f\"Spouse_{i}_Birth\"\n",
    "    death_column = f\"Spouse_{i}_Death\"\n",
    "    occupation_column = f\"Spouse_{i}_Occupation\"\n",
    "    relationship_date_column = f\"Spouse_{i}_Relationship_Date\"\n",
    "    \n",
    "    # Add constructed columns to the list\n",
    "    spouse_columns.extend([gender_column, main_name_column, forenames_column, \n",
    "                           birth_column, death_column, occupation_column, \n",
    "                           relationship_date_column])\n",
    "\n",
    "# Iterate through all spouse columns and get all unique columns\n",
    "for column in spouse_columns:\n",
    "    all_columns.append(column)\n",
    "\n",
    "# Get unique columns and sort them\n",
    "all_columns = sorted(set(all_columns))\n",
    "\n",
    "# Define a list to store only spouse name related columns\n",
    "spouse_name_columns = []\n",
    "\n",
    "# Define a list to store columns not related to names\n",
    "non_name_related_columns = []\n",
    "\n",
    "# Filter out spouse name related columns\n",
    "for column in all_columns:\n",
    "    if \"MainName\" in column or \"ForeNames\" in column:\n",
    "        spouse_name_columns.append(column)\n",
    "    else:\n",
    "        non_name_related_columns.append(column)\n",
    "\n",
    "# Print the spouse name related columns\n",
    "print(\"Spouse Name Related Columns:\")\n",
    "for column in spouse_name_columns:\n",
    "    print(column)\n",
    "\n",
    "# Print the columns not related to the names\n",
    "print(\"\\nColumns not related to names:\")\n",
    "for column in non_name_related_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2122c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of columns not related to names\n",
    "columns_to_drop = [\n",
    "    f\"Spouse_{i}_Birth\" for i in range(1, 13)\n",
    "] + [\n",
    "    f\"Spouse_{i}_Death\" for i in range(1, 13)\n",
    "] + [\n",
    "    f\"Spouse_{i}_Gender\" for i in range(1, 13)\n",
    "] + [\n",
    "    f\"Spouse_{i}_Occupation\" for i in range(1, 13)\n",
    "] + [\n",
    "    f\"Spouse_{i}_Relationship_Date\" for i in range(1, 13)\n",
    "]\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a18455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Author_Forenames Author_Surname Gender     MainName        foreNames  \\\n",
      "0             Veronica          Kelly      f      Brayton             Lily   \n",
      "1            Maggie B.           Gale      f      Collier        Constance   \n",
      "2                  Rob          Steen      m        Close           Dennis   \n",
      "3             Rosemary          Scott      m       Barham   Richard Harris   \n",
      "4                R. K.           Webb      m       Barham    Thomas Foster   \n",
      "...                ...            ...    ...          ...              ...   \n",
      "39512          Todd M.       Thompson      m         Kerr      David Allan   \n",
      "39513             Gill      Coleridge      m      Kington  Miles Beresford   \n",
      "39514            Tessa        Murdoch      m  Fourdrinier             Paul   \n",
      "39515        Ronald T.         Raines      m      Knowles   Jeremy Randall   \n",
      "39516              NaN            NaN      m        Laing     John Maurice   \n",
      "\n",
      "                       ShortForm       Birth         Birth Place - Street  \\\n",
      "0      Brayton, Elizabeth [Lily]  1876-06-23      White House, Stony Lane   \n",
      "1                            NaN  1878-01-22  2 Argyll Villas, Grove Road   \n",
      "2                            NaN  1931-02-24                 8 New Street   \n",
      "3                            NaN  1788-12-06            61 Burgate Street   \n",
      "4                            NaN  1794-09-10                          NaN   \n",
      "...                          ...         ...                          ...   \n",
      "39512                        NaN  1945-05-16                          NaN   \n",
      "39513                        NaN  1941-05-13                          NaN   \n",
      "39514                        NaN        1698                          NaN   \n",
      "39515                        NaN  1935-04-28                1 Murray Road   \n",
      "39516                        NaN  1918-02-01           42 St James's Road   \n",
      "\n",
      "      Birth Place - Town Birth Place - City  ... Spouse_9_MainName  \\\n",
      "0                Hindley                NaN  ...               NaN   \n",
      "1            New Windsor                NaN  ...               NaN   \n",
      "2                 Rawdon                NaN  ...               NaN   \n",
      "3             Canterbury                NaN  ...               NaN   \n",
      "4                 Hendon                NaN  ...               NaN   \n",
      "...                  ...                ...  ...               ...   \n",
      "39512                NaN                NaN  ...               NaN   \n",
      "39513        Downpatrick                NaN  ...               NaN   \n",
      "39514          Groningen                NaN  ...               NaN   \n",
      "39515              Rugby                NaN  ...               NaN   \n",
      "39516           Carlisle                NaN  ...               NaN   \n",
      "\n",
      "      Spouse_9_ForeNames Spouse_10_MainName Spouse_10_ForeNames  \\\n",
      "0                    NaN                NaN                 NaN   \n",
      "1                    NaN                NaN                 NaN   \n",
      "2                    NaN                NaN                 NaN   \n",
      "3                    NaN                NaN                 NaN   \n",
      "4                    NaN                NaN                 NaN   \n",
      "...                  ...                ...                 ...   \n",
      "39512                NaN                NaN                 NaN   \n",
      "39513                NaN                NaN                 NaN   \n",
      "39514                NaN                NaN                 NaN   \n",
      "39515                NaN                NaN                 NaN   \n",
      "39516                NaN                NaN                 NaN   \n",
      "\n",
      "      Spouse_11_MainName Spouse_11_ForeNames Spouse_12_MainName  \\\n",
      "0                    NaN                 NaN                NaN   \n",
      "1                    NaN                 NaN                NaN   \n",
      "2                    NaN                 NaN                NaN   \n",
      "3                    NaN                 NaN                NaN   \n",
      "4                    NaN                 NaN                NaN   \n",
      "...                  ...                 ...                ...   \n",
      "39512                NaN                 NaN                NaN   \n",
      "39513                NaN                 NaN                NaN   \n",
      "39514                NaN                 NaN                NaN   \n",
      "39515                NaN                 NaN                NaN   \n",
      "39516                NaN                 NaN                NaN   \n",
      "\n",
      "      Spouse_12_ForeNames                                       Spouse_Names  \\\n",
      "0                     NaN  Thomas Stange Heiss Oscar Asche, Douglas Chalm...   \n",
      "1                     NaN                                                      \n",
      "2                     NaN                                                      \n",
      "3                     NaN                                     Caroline Smart   \n",
      "4                     NaN                   Sarah Garratt, Margaret Henryson   \n",
      "...                   ...                                                ...   \n",
      "39512                 NaN                             Gun-Marianne Holmstrom   \n",
      "39513                 NaN               Sarah Paine, Hilary Caroline Maynard   \n",
      "39514                 NaN                                   Susanne Grolleau   \n",
      "39515                 NaN                                 Jane Sheldon Davis   \n",
      "39516                 NaN                              Hilda Violet Richards   \n",
      "\n",
      "      No_of_Spouse  \n",
      "0                2  \n",
      "1                0  \n",
      "2                0  \n",
      "3                1  \n",
      "4                2  \n",
      "...            ...  \n",
      "39512            1  \n",
      "39513            2  \n",
      "39514            1  \n",
      "39515            1  \n",
      "39516            1  \n",
      "\n",
      "[39517 rows x 78 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store spouse names and count\n",
    "spouse_names = []\n",
    "no_of_spouses = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Initialize variables for spouse names and count for current row\n",
    "    current_spouse_names = []\n",
    "    current_no_of_spouses = 0\n",
    "    \n",
    "    # Iterate through spouse-related columns\n",
    "    for i in range(1, 13):\n",
    "        main_name_col = f\"Spouse_{i}_MainName\"\n",
    "        forenames_col = f\"Spouse_{i}_ForeNames\"\n",
    "        \n",
    "        # Check if both MainName and ForeNames are available\n",
    "        if pd.notna(row[main_name_col]) and pd.notna(row[forenames_col]):\n",
    "            # Concatenate MainName and ForeNames with a space in between\n",
    "            current_spouse_names.append(f\"{row[forenames_col]} {row[main_name_col]}\")\n",
    "        # Check if only MainName is available\n",
    "        elif pd.notna(row[main_name_col]):\n",
    "            current_spouse_names.append(row[main_name_col])\n",
    "        # Check if only ForeNames is available\n",
    "        elif pd.notna(row[forenames_col]):\n",
    "            current_spouse_names.append(row[forenames_col])\n",
    "        \n",
    "        # Increment spouse count if either MainName or ForeNames is available\n",
    "        if pd.notna(row[main_name_col]) or pd.notna(row[forenames_col]):\n",
    "            current_no_of_spouses += 1\n",
    "    \n",
    "    # Append the concatenated spouse names and count for current row\n",
    "    spouse_names.append(\", \".join(current_spouse_names))\n",
    "    no_of_spouses.append(current_no_of_spouses)\n",
    "\n",
    "# Create a temporary DataFrame to hold the computed values\n",
    "temp_df = pd.DataFrame({\n",
    "    'Spouse_Names': spouse_names,\n",
    "    'No_of_Spouse': no_of_spouses\n",
    "}, index=df.index)\n",
    "\n",
    "# Concatenate the original DataFrame with the temporary DataFrame\n",
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e7f787",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Spouse_1_MainName', 'Spouse_1_ForeNames'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     forenames_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpouse_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ForeNames\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Drop the old spouse-related columns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[main_name_col, forenames_col], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the DataFrame to verify the changes\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Spouse_1_MainName', 'Spouse_1_ForeNames'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Iterate through spouse-related columns\n",
    "for i in range(1, 13):\n",
    "    main_name_col = f\"Spouse_{i}_MainName\"\n",
    "    forenames_col = f\"Spouse_{i}_ForeNames\"\n",
    "    \n",
    "    # Drop the old spouse-related columns\n",
    "    df.drop(columns=[main_name_col, forenames_col], inplace=True)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba61b42",
   "metadata": {},
   "source": [
    "## Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e13af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Author_Forenames Author_Surname Gender     MainName        foreNames  \\\n",
      "0             Veronica          Kelly      f      Brayton             Lily   \n",
      "1            Maggie B.           Gale      f      Collier        Constance   \n",
      "2                  Rob          Steen      m        Close           Dennis   \n",
      "3             Rosemary          Scott      m       Barham   Richard Harris   \n",
      "4                R. K.           Webb      m       Barham    Thomas Foster   \n",
      "...                ...            ...    ...          ...              ...   \n",
      "39512          Todd M.       Thompson      m         Kerr      David Allan   \n",
      "39513             Gill      Coleridge      m      Kington  Miles Beresford   \n",
      "39514            Tessa        Murdoch      m  Fourdrinier             Paul   \n",
      "39515        Ronald T.         Raines      m      Knowles   Jeremy Randall   \n",
      "39516              NaN            NaN      m        Laing     John Maurice   \n",
      "\n",
      "                       ShortForm       Birth         Birth Place - Street  \\\n",
      "0      Brayton, Elizabeth [Lily]  1876-06-23      White House, Stony Lane   \n",
      "1                            NaN  1878-01-22  2 Argyll Villas, Grove Road   \n",
      "2                            NaN  1931-02-24                 8 New Street   \n",
      "3                            NaN  1788-12-06            61 Burgate Street   \n",
      "4                            NaN  1794-09-10                          NaN   \n",
      "...                          ...         ...                          ...   \n",
      "39512                        NaN  1945-05-16                          NaN   \n",
      "39513                        NaN  1941-05-13                          NaN   \n",
      "39514                        NaN        1698                          NaN   \n",
      "39515                        NaN  1935-04-28                1 Murray Road   \n",
      "39516                        NaN  1918-02-01           42 St James's Road   \n",
      "\n",
      "      Birth Place - Town Birth Place - City  ...  \\\n",
      "0                Hindley                NaN  ...   \n",
      "1            New Windsor                NaN  ...   \n",
      "2                 Rawdon                NaN  ...   \n",
      "3             Canterbury                NaN  ...   \n",
      "4                 Hendon                NaN  ...   \n",
      "...                  ...                ...  ...   \n",
      "39512                NaN                NaN  ...   \n",
      "39513        Downpatrick                NaN  ...   \n",
      "39514          Groningen                NaN  ...   \n",
      "39515              Rugby                NaN  ...   \n",
      "39516           Carlisle                NaN  ...   \n",
      "\n",
      "                                    College_1 College_1_City  \\\n",
      "0                                         NaN            NaN   \n",
      "1                                         NaN            NaN   \n",
      "2                                         NaN            NaN   \n",
      "3                           Brasenose College         Oxford   \n",
      "4                             Queens' College      Cambridge   \n",
      "...                                       ...            ...   \n",
      "39512  School of Oriental and African Studies            NaN   \n",
      "39513                         Trinity College         Oxford   \n",
      "39514                                     NaN            NaN   \n",
      "39515                         Balliol College         Oxford   \n",
      "39516                                     NaN            NaN   \n",
      "\n",
      "      College_1_Country School_2 School_2_City School_2_Country  \\\n",
      "0                   NaN      NaN           NaN              NaN   \n",
      "1                   NaN      NaN           NaN              NaN   \n",
      "2                   NaN      NaN           NaN              NaN   \n",
      "3               England      NaN           NaN              NaN   \n",
      "4               England      NaN           NaN              NaN   \n",
      "...                 ...      ...           ...              ...   \n",
      "39512           England      NaN           NaN              NaN   \n",
      "39513           England      NaN           NaN              NaN   \n",
      "39514               NaN      NaN           NaN              NaN   \n",
      "39515           England      NaN           NaN              NaN   \n",
      "39516               NaN      NaN           NaN              NaN   \n",
      "\n",
      "                                            Spouse_Names No_of_Spouse  \\\n",
      "0      Thomas Stange Heiss Oscar Asche, Douglas Chalm...            2   \n",
      "1                                                                   0   \n",
      "2                                                                   0   \n",
      "3                                         Caroline Smart            1   \n",
      "4                       Sarah Garratt, Margaret Henryson            2   \n",
      "...                                                  ...          ...   \n",
      "39512                             Gun-Marianne Holmstrom            1   \n",
      "39513               Sarah Paine, Hilary Caroline Maynard            2   \n",
      "39514                                   Susanne Grolleau            1   \n",
      "39515                                 Jane Sheldon Davis            1   \n",
      "39516                              Hilda Violet Richards            1   \n",
      "\n",
      "                          Schools No_of_Schools  \n",
      "0                                             0  \n",
      "1                                             0  \n",
      "2      Aireborough Grammar School             1  \n",
      "3                St Paul's School             1  \n",
      "4                                             0  \n",
      "...                           ...           ...  \n",
      "39512            Mill Hill School             1  \n",
      "39513             Trinity College             1  \n",
      "39514                                         0  \n",
      "39515     Magdalen College School             1  \n",
      "39516         St Lawrence College             1  \n",
      "\n",
      "[39517 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store school names and counts\n",
    "schools = []\n",
    "no_of_schools = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Initialize variables for school names and count for current row\n",
    "    current_schools = []\n",
    "    current_no_of_schools = 0\n",
    "    \n",
    "    # Iterate through school-related columns\n",
    "    for i in range(1, 3):\n",
    "        school_col = f\"School_{i}\"\n",
    "        \n",
    "        # Check if school name is available\n",
    "        if pd.notna(row[school_col]):\n",
    "            # Append school name to the list\n",
    "            current_schools.append(row[school_col])\n",
    "            current_no_of_schools += 1\n",
    "    \n",
    "    # Append the concatenated school names and count for current row\n",
    "    schools.append(\", \".join(current_schools))\n",
    "    no_of_schools.append(current_no_of_schools)\n",
    "\n",
    "# Create a temporary DataFrame to hold the computed values\n",
    "temp_df_school = pd.DataFrame({\n",
    "    'Schools': schools,\n",
    "    'No_of_Schools': no_of_schools\n",
    "}, index=df.index)\n",
    "\n",
    "# Concatenate the original DataFrame with the temporary DataFrame\n",
    "df = pd.concat([df, temp_df_school], axis=1)\n",
    "\n",
    "# # Drop unnecessary city and country columns\n",
    "# columns_to_drop = ['School_1_City', 'School_1_Country', 'School_2_City', 'School_2_Country','College_1_City','College_1_Country']\n",
    "# df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c23a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = ['School_1', 'School_2']\n",
    "# Drop the columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601deb6e",
   "metadata": {},
   "source": [
    "## Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540b536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = ['Author_Forenames', 'Author_Surname']\n",
    "# Drop the columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c4fb1",
   "metadata": {},
   "source": [
    "## Parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b26030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = ['Father_Gender', 'Mother_Gender']\n",
    "# Drop the columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155516d",
   "metadata": {},
   "source": [
    "## Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db60300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "Gender: 2032\n",
      "MainName: 1294\n",
      "foreNames: 3092\n",
      "ShortForm: 39472\n",
      "Birth: 11081\n",
      "Birth Place - Street: 31193\n",
      "Birth Place - Town: 17498\n",
      "Birth Place - City: 39186\n",
      "Birth Place - County: 15136\n",
      "Birth Place - Country: 12915\n",
      "Death: 5328\n",
      "Death Place - Street: 25112\n",
      "Death Place - Town: 15291\n",
      "Death Place - City: 39227\n",
      "Death Place - County: 12768\n",
      "Death Place - Country: 10317\n",
      "Occupation: 5801\n",
      "Alternative Names: 22932\n",
      "Father_MainName: 7980\n",
      "Father_ForeNames: 9679\n",
      "Father_Birth: 38972\n",
      "Father_Death: 38932\n",
      "Father_Occupation: 13632\n",
      "Mother_MainName: 13390\n",
      "Mother_ForeNames: 14450\n",
      "Mother_Birth: 39020\n",
      "Mother_Death: 38985\n",
      "Mother_Occupation: 37138\n",
      "Residence_Date: 39068\n",
      "Residence_Town: 15432\n",
      "Residence_Street: 28042\n",
      "Residence_City: 39219\n",
      "Residence_County: 9205\n",
      "Residence_Country: 5804\n",
      "Taxonomy_Terms: 1817\n",
      "Institutions: 10647\n",
      "Wealth_Amount: 25612\n",
      "Wealth_Probate_Date: 23995\n",
      "Wealth_Abbreviation: 39506\n",
      "School_1_City: 39281\n",
      "School_1_Country: 25746\n",
      "College_1: 24585\n",
      "College_1_City: 26612\n",
      "College_1_Country: 24847\n",
      "School_2_City: 39516\n",
      "School_2_Country: 39516\n",
      "Spouse_Names: 0\n",
      "No_of_Spouse: 0\n",
      "Schools: 0\n",
      "No_of_Schools: 0\n"
     ]
    }
   ],
   "source": [
    "# Display missing values for all columns\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "for column, missing_count in missing_values.items():\n",
    "    print(f\"{column}: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c5bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = ['ShortForm','Birth Place - Street','Death Place - Street','Father_Birth','Father_Death','Mother_Birth','Mother_Death']\n",
    "# Drop the columns from the DataFrame\n",
    "df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cbdae1",
   "metadata": {},
   "source": [
    "## Date Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0032831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Birth</th>\n",
       "      <th>Death</th>\n",
       "      <th>Birth_Year</th>\n",
       "      <th>Death_Year</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1876-06-23</td>\n",
       "      <td>1953-04-30</td>\n",
       "      <td>1876.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1878-01-22</td>\n",
       "      <td>1955-04-25</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1931-02-24</td>\n",
       "      <td>2015-09-13</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1788-12-06</td>\n",
       "      <td>1845-06-17</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794-09-10</td>\n",
       "      <td>1869-03-03</td>\n",
       "      <td>1794.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>1945-05-16</td>\n",
       "      <td>2008-04-14</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39513</th>\n",
       "      <td>1941-05-13</td>\n",
       "      <td>2008-01-30</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39514</th>\n",
       "      <td>1698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1698.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39515</th>\n",
       "      <td>1935-04-28</td>\n",
       "      <td>2008-04-03</td>\n",
       "      <td>1935.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39516</th>\n",
       "      <td>1918-02-01</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>1918.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39517 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Birth       Death  Birth_Year  Death_Year   Age\n",
       "0      1876-06-23  1953-04-30      1876.0      1953.0  77.0\n",
       "1      1878-01-22  1955-04-25      1878.0      1955.0  77.0\n",
       "2      1931-02-24  2015-09-13      1931.0      2015.0  84.0\n",
       "3      1788-12-06  1845-06-17      1788.0      1845.0  57.0\n",
       "4      1794-09-10  1869-03-03      1794.0      1869.0  75.0\n",
       "...           ...         ...         ...         ...   ...\n",
       "39512  1945-05-16  2008-04-14      1945.0      2008.0  63.0\n",
       "39513  1941-05-13  2008-01-30      1941.0      2008.0  67.0\n",
       "39514        1698         NaN      1698.0         NaN   NaN\n",
       "39515  1935-04-28  2008-04-03      1935.0      2008.0  73.0\n",
       "39516  1918-02-01  2008-02-22      1918.0      2008.0  90.0\n",
       "\n",
       "[39517 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A more robust function to extract the year from various date formats\n",
    "def extract_year(date_str):\n",
    "    if pd.isnull(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        # First attempt to parse the date using to_datetime, which will correctly\n",
    "        # interpret different date formats like \"YYYY-MM-DD\" or \"MM/DD/YYYY\".\n",
    "        return pd.to_datetime(date_str, errors='coerce').year\n",
    "    except ValueError:\n",
    "        pass  # Ignore the ValueError and attempt further processing\n",
    "    \n",
    "    # If to_datetime failed, manually parse the string\n",
    "    try:\n",
    "        parts = date_str.replace('-', '/').split('/')\n",
    "        # If the length of the first part is 4, it's likely the year\n",
    "        if len(parts[0]) == 4:\n",
    "            return int(parts[0])\n",
    "        # If the length of the last part is 4, it's likely the year\n",
    "        elif len(parts[-1]) == 4:\n",
    "            return int(parts[-1])\n",
    "        else:\n",
    "            # If year is not found, return None\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # In case of any other unexpected format or error\n",
    "        return None\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame after loading the CSV:\n",
    "df['Birth_Year'] = df['Birth'].apply(extract_year)\n",
    "df['Death_Year'] = df['Death'].apply(extract_year)\n",
    "\n",
    "# Calculate the age by subtracting the Birth year from the Death year\n",
    "df['Age'] = df['Death_Year'] - df['Birth_Year']\n",
    "\n",
    "# Display the DataFrame to verify the results\n",
    "df[['Birth', 'Death', 'Birth_Year', 'Death_Year', 'Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b62f88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the mainname and forename columns\n",
    "df['Name'] = df['foreNames'] + ' ' + df['MainName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbcdca",
   "metadata": {},
   "source": [
    "# 3. Saving the processed data to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fd919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Final_Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d023be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
